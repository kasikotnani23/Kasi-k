{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kasikotnani23/Kasi-k/blob/main/Working_with_DF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjuAVgrxmIo3"
      },
      "source": [
        "# **Working with DF (DataFrame)**\n",
        "\n",
        "**`Udemy Course: Best Hands-on Big Data Practices and Use Cases using PySpark`**\n",
        "\n",
        "**`Author: Amin Karami (PhD, FHEA)`**\n",
        "\n",
        "---\n",
        "\n",
        "**DataFrame (DF)**: Schema (named columns) + declarative language. A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. It is very efficient for strucutred data.\n",
        "\n",
        "source: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
        "\n",
        "source: https://spark.apache.org/docs/latest/api/python/reference/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LWTJaC8mHL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc379e1a-4182-4d1e-8abe-0b8c00083095"
      },
      "source": [
        "########## ONLY in Colab ##########\n",
        "!pip3 install pyspark\n",
        "########## ONLY in Colab ##########"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=6e6c34371e8c14500f125a74f8e5cf14608eee6dc8ec57588422bda1aa4fb084\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linking with Spark (https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "e3pTfRiwTMeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Create DF and Basic Operations**"
      ],
      "metadata": {
        "id": "quQ_GBpgWLRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create/Load DF: (Spark automatically scans through the files and infers the schema of the dataset)\n",
        "# data source: https://www.kaggle.com/thec03u5/fifa-18-demo-player-dataset\n",
        "\n",
        "df1 = spark.read.format(\"csv\").load(\"CompleteDataset.csv\", inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "1n39Bv24XHjt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show data:\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "b8aOYoMLX7Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many partitions in DF?\n",
        "df1.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "9EffFOyTYC18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase/Desrease the partitions in DF\n",
        "df2 = df1.repartition(4)\n",
        "df2.rdd.getNumPartitions()"
      ],
      "metadata": {
        "id": "YBP0nzanB1Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show DF\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "N0sR-ffdCKuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename Columns and Amend NULLs:\n",
        "df2 = df2.withColumnRenamed(\"_c0\", \"ID\") \\\n",
        "    .withColumnRenamed(\"Ball control\", \"Ball_Control\")\\\n",
        "    .withColumnRenamed(\"Sliding tackle\", \"Sliding_Tackle\")\n",
        "\n",
        "df2.na.fill({\"RAM\": 10, \"RB\": 1}).show()"
      ],
      "metadata": {
        "id": "Yp9QWyKcDQZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (SELECT):\n",
        "df2.select(\"Name\",\"Overall\").distinct().show()"
      ],
      "metadata": {
        "id": "Vf9eiG93HbKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (FILTER):\n",
        "df2.filter(df2[\"Overall\"] > 70).show()"
      ],
      "metadata": {
        "id": "QlLKM3krHyWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (FILTER):\n",
        "df2.select(\"Overall\", \"Name\", \"Age\").where(df2[\"Overall\"]>70).show()"
      ],
      "metadata": {
        "id": "7b94hjJuIDj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation (FILTER):\n",
        "df2.where(df2[\"Overall\"]>70).groupBy(\"Age\").count().sort(\"Age\").show()"
      ],
      "metadata": {
        "id": "vvnmUbd1IgKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results:\n",
        "df2_result = df2.where(df2[\"Overall\"]>70).groupBy(\"Age\").count().sort(\"Age\")\n",
        "\n",
        "pandas_df = df2_result.toPandas()\n",
        "pandas_df.plot(x = \"Age\", y = \"count\", kind = \"bar\")\n"
      ],
      "metadata": {
        "id": "qyoxZNRDJYJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas_df.sort_values(by=\"count\", ascending=False).plot(x = \"Age\", y = \"count\", kind = \"bar\")"
      ],
      "metadata": {
        "id": "xL4ac6O7wZvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Advanced DF Operations: Spark SQL and UDF**"
      ],
      "metadata": {
        "id": "EGi2zdncaoHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark SQL (Register the DF using a local temporary view):\n",
        "\n",
        "df2.createOrReplaceTempView(\"df_football\")"
      ],
      "metadata": {
        "id": "YhJzLq8XaoCL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL Query:\n",
        "\n",
        "sql_query = \"\"\" SELECT Age, count(*) as Count\n",
        "                FROM df_football\n",
        "                WHERE Overall > 70\n",
        "                GROUP BY Age\n",
        "                ORDER BY Age \"\"\"\n",
        "\n",
        "result = spark.sql(sql_query)\n",
        "result.show()"
      ],
      "metadata": {
        "id": "bIKu4KMrdt1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UDF (User Defined Functions):\n",
        "def uppercase_converter(record):\n",
        "  if record is not None:\n",
        "    if len(record) > 10:\n",
        "      return record.upper()\n",
        "    else:\n",
        "      return record.lower()\n",
        "\n",
        "# register the DF\n",
        "df2.createOrReplaceTempView(\"UDF_football\")\n",
        "\n",
        "# register the function\n",
        "spark.udf.register(\"UPPER\", uppercase_converter)\n",
        "\n",
        "# use the UDF in SQL\n",
        "sql_query = \"SELECT Age, UPPER(Name) as Name, UPPER(Club) as Club FROM UDF_football\"\n",
        "\n",
        "result = spark.sql(sql_query)\n",
        "result.show()\n"
      ],
      "metadata": {
        "id": "OgP3auupaaTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}